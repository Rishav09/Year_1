{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms.functional as TF\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import os\n",
    "import random\n",
    "import glob\n",
    "from PIL import Image\n",
    "from scipy.ndimage.filters import gaussian_filter\n",
    "from scipy.ndimage.interpolation import map_coordinates\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import torch.utils.data as data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tempfile\n",
    "import torch.distributed as dist\n",
    "import torch.multiprocessing as mp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "\n",
    "def setup(rank,world_size):\n",
    "    os.environ['MASTER_ADDR'] = 'localhost'\n",
    "    os.environ['MASTER_PORT'] = '12355'\n",
    "    \n",
    "    dist.init_process_group(\"gloo\",rank = rank,world_size=world_size)\n",
    "    torch.manual_seed(42)\n",
    "\n",
    "def cleanup():\n",
    "    dist.destroy_process_group()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4.])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_dir = '/Users/swastik/ophthalmology/Datasets/Drive_Dataset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data as data\n",
    "\n",
    "class DataLoaderSegmentation(data.Dataset):\n",
    "    def __init__(self,folder_path,transform = None):\n",
    "        super(DataLoaderSegmentation, self).__init__()\n",
    "        self.img_files = glob.glob(os.path.join(folder_path,'images','*.tif'))\n",
    "        self.mask_files = glob.glob(os.path.join(folder_path,'new_mask','*.bmp'))\n",
    "        self.transforms = transform\n",
    "        #for img_path in img_files:\n",
    "         #   self.mask_files.append(os.path.join(folder_path,'masks',os.path.basename(img_path))\n",
    "         \n",
    "    def mask_to_class(self,mask):\n",
    "        target = torch.from_numpy(mask)\n",
    "        assert target.shape[2] ==3\n",
    "        h,w = target.shape[0],target.shape[1]\n",
    "        masks = torch.empty(h, w, dtype=torch.long)\n",
    "        colors = torch.unique(target.view(-1,target.size(2)),dim=0).numpy()\n",
    "        target = target.permute(2, 0, 1).contiguous()\n",
    "        mapping = {tuple(c): t for c, t in zip(colors.tolist(), range(len(colors)))}\n",
    "        for k in mapping:\n",
    "            idx = (target==torch.tensor(k, dtype=torch.uint8).unsqueeze(1).unsqueeze(2))\n",
    "            validx = (idx.sum(0) == 3) \n",
    "            masks[validx] = torch.tensor(mapping[k], dtype=torch.long)\n",
    "        return masks\n",
    "    \n",
    "    def elastic_transform_nearest(self,image, alpha=1000, sigma=20, spline_order=0, mode='nearest', random_state=np.random):\n",
    "        \n",
    "        image = np.array(image)\n",
    "       # assert image.ndim == 3\n",
    "        shape = image.shape[:2]\n",
    "\n",
    "        dx = gaussian_filter((random_state.rand(*shape) * 2 - 1),\n",
    "                         sigma, mode=\"constant\", cval=0) * alpha\n",
    "        dy = gaussian_filter((random_state.rand(*shape) * 2 - 1),\n",
    "                      sigma, mode=\"constant\", cval=0) * alpha\n",
    "\n",
    "        x, y = np.meshgrid(np.arange(shape[0]), np.arange(shape[1]), indexing='ij')\n",
    "        indices = [np.reshape(x + dx, (-1, 1)), np.reshape(y + dy, (-1, 1))]\n",
    "        result = np.empty_like(image)\n",
    "        for i in range(image.shape[2]):\n",
    "            result[:, :, i] = map_coordinates(\n",
    "            image[:, :, i], indices, order=spline_order, mode=mode).reshape(shape)\n",
    "        result = Image.fromarray(result)\n",
    "        return result\n",
    "    \n",
    "    def elastic_transform_bilinear(self,image, alpha=1000, sigma=20, spline_order=1, mode='nearest', random_state=np.random):\n",
    "        \n",
    "\n",
    "        image = np.array(image)\n",
    "        #assert image.ndim == 3\n",
    "        shape = image.shape[:2]\n",
    "        dx = gaussian_filter((random_state.rand(*shape) * 2 - 1),\n",
    "                         sigma, mode=\"constant\", cval=0) * alpha\n",
    "        dy = gaussian_filter((random_state.rand(*shape) * 2 - 1),\n",
    "                         sigma, mode=\"constant\", cval=0) * alpha\n",
    "\n",
    "        x, y = np.meshgrid(np.arange(shape[0]), np.arange(shape[1]), indexing='ij')\n",
    "        indices = [np.reshape(x + dx, (-1, 1)), np.reshape(y + dy, (-1, 1))]\n",
    "        result = np.empty_like(image)\n",
    "        for i in range(image.shape[2]):\n",
    "            result[:, :, i] = map_coordinates(\n",
    "            image[:, :, i], indices, order=spline_order, mode=mode).reshape(shape)\n",
    "        result = Image.fromarray(result)\n",
    "        return result\n",
    "    \n",
    "    def transform(self,image,mask):\n",
    "        i, j, h, w = transforms.RandomCrop.get_params(\n",
    "        image, output_size=(512, 512))\n",
    "        image = TF.crop(image, i, j, h, w)\n",
    "        mask = TF.crop(mask, i, j, h, w)\n",
    "        \n",
    "        #image = TF.Lambda(gaussian_blur),\n",
    "       # mask = \n",
    "        #image = TF.Lambda(elastic_transform)\n",
    "        # Random horizontal flipping\n",
    "        #image = transforms.transforms.Lambda(gaussian_blur)\n",
    "        if random.random() > 0.5:\n",
    "            image = TF.hflip(image)\n",
    "            mask = TF.hflip(mask)\n",
    "        \n",
    "        image = TF.rotate(image,90)\n",
    "        mask = TF.rotate(mask,90)\n",
    "        image = TF.rotate(image,180)\n",
    "        mask = TF.rotate(mask,180)\n",
    "        image = TF.rotate(image,270)\n",
    "        mask = TF.rotate(mask,270)\n",
    "\n",
    "        # Transform to tensor\n",
    "        #image = TF.to_tensor(image)\n",
    "#         mask = TF.to_tensor(mask)\n",
    "        return image, mask\n",
    "     \n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        img_path = self.img_files[index]\n",
    "        mask_path = self.mask_files[index]\n",
    "        data = Image.open(img_path)\n",
    "        label = Image.open(mask_path)\n",
    "       # label = np.array(label)\n",
    "        data = self.elastic_transform_bilinear(data)\n",
    "        label = self.elastic_transform_nearest(label)\n",
    "        data,label = self.transform(data,label)\n",
    "        label = np.array(label)\n",
    "        data = np.array(data)\n",
    "        #label = np.transpose(label,(2,0,1))\n",
    "        mask = self.mask_to_class(label)\n",
    "        if transforms is not None:\n",
    "             data = self.transforms(data)\n",
    "        return data,mask\n",
    "       # return data, torch.from_numpy(label).long()\n",
    "           \n",
    "    def __len__(self):\n",
    "        return len(self.img_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.segmentation import find_boundaries\n",
    "\n",
    "w0 = 10\n",
    "sigma = 5\n",
    "\n",
    "def make_weight_map(masks):\n",
    "    \"\"\"\n",
    "    Generate the weight maps as specified in the UNet paper\n",
    "    for a set of binary masks.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    masks: array-like\n",
    "        A 3D array of shape (n_masks, image_height, image_width),\n",
    "        where each slice of the matrix along the 0th axis represents one binary mask.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    array-like\n",
    "        A 2D array of shape (image_height, image_width)\n",
    "    \n",
    "    \"\"\"\n",
    "    masks = masks.numpy()\n",
    "    nrows, ncols = masks.shape[1:]\n",
    "    masks = (masks > 0).astype(int)\n",
    "    distMap = np.zeros((nrows * ncols, masks.shape[0]))\n",
    "    X1, Y1 = np.meshgrid(np.arange(nrows), np.arange(ncols))\n",
    "    X1, Y1 = np.c_[X1.ravel(), Y1.ravel()].T\n",
    "    for i, mask in enumerate(masks):\n",
    "        # find the boundary of each mask,\n",
    "        # compute the distance of each pixel from this boundary\n",
    "        bounds = find_boundaries(mask, mode='inner')\n",
    "        X2, Y2 = np.nonzero(bounds)\n",
    "        xSum = (X2.reshape(-1, 1) - X1.reshape(1, -1)) ** 2\n",
    "        ySum = (Y2.reshape(-1, 1) - Y1.reshape(1, -1)) ** 2\n",
    "        distMap[:, i] = np.sqrt(xSum + ySum).min(axis=0)\n",
    "    ix = np.arange(distMap.shape[0])\n",
    "    if distMap.shape[1] == 1:\n",
    "        d1 = distMap.ravel()\n",
    "        border_loss_map = w0 * np.exp((-1 * (d1) ** 2) / (2 * (sigma ** 2)))\n",
    "    else:\n",
    "        if distMap.shape[1] == 2:\n",
    "            d1_ix, d2_ix = np.argpartition(distMap, 1, axis=1)[:, :2].T\n",
    "        else:\n",
    "            d1_ix, d2_ix = np.argpartition(distMap, 2, axis=1)[:, :2].T\n",
    "        d1 = distMap[ix, d1_ix]\n",
    "        d2 = distMap[ix, d2_ix]\n",
    "        border_loss_map = w0 * np.exp((-1 * (d1 + d2) ** 2) / (2 * (sigma ** 2)))\n",
    "    xBLoss = np.zeros((nrows, ncols))\n",
    "    xBLoss[X1, Y1] = border_loss_map\n",
    "    # class weight map\n",
    "    loss = np.zeros((nrows, ncols))\n",
    "    w_1 = 1 - masks.sum() / loss.size\n",
    "    w_0 = 1 - w_1\n",
    "    loss[masks.sum(0) == 1] = w_1\n",
    "    loss[masks.sum(0) == 0] = w_0\n",
    "    ZZ = xBLoss + loss\n",
    "    ZZ = torch.from_numpy(ZZ)\n",
    "    ZZ = ZZ.type(torch.float)\n",
    "    return ZZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No gpu\n"
     ]
    }
   ],
   "source": [
    "train_on_gpu = torch.cuda.is_available()\n",
    "if train_on_gpu:\n",
    "    print(\"Gpu is being used\")\n",
    "else:\n",
    "    print(\"No gpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = DataLoaderSegmentation(path_dir,transform = transforms.ToTensor())\n",
    "train_loader = DataLoader(dataset=dataset,batch_size=batch_size,shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3, 512, 512])\n",
      "torch.Size([5, 512, 512])\n"
     ]
    }
   ],
   "source": [
    "for image,labels in iter(train_loader):\n",
    "#     labels = labels.view(labels.shape[:2],-1)\n",
    "    print(image.shape)\n",
    "    print(labels.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: tensor([0.5970, 0.3206, 0.1899])\n",
      "Std Dev: tensor([0.2829, 0.1522, 0.0833])\n"
     ]
    }
   ],
   "source": [
    "train_mean = []\n",
    "train_std = []\n",
    "\n",
    "for i,image in enumerate(train_loader,0):\n",
    "#      image[0].shape()\n",
    "    numpy_image = image[0].numpy()\n",
    "    batch_mean = np.mean(numpy_image, axis=(0, 2, 3))\n",
    "    batch_std = np.std(numpy_image, axis=(0, 2, 3))\n",
    "    \n",
    "    train_mean.append(batch_mean)\n",
    "    train_std.append(batch_std)\n",
    "    \n",
    "train_mean = torch.tensor(np.mean(train_mean, axis=0))\n",
    "train_std = torch.tensor(np.mean(train_std, axis=0))\n",
    "\n",
    "print('Mean:', train_mean)\n",
    "print('Std Dev:', train_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "data_transforms = transforms.Compose([\n",
    "                                 transforms.ToTensor(),\n",
    "                                 transforms.Normalize(mean=train_mean, std=train_std)\n",
    "                               ])\n",
    "final_dataset = DataLoaderSegmentation(path_dir,transform = data_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_split = 0.15\n",
    "shuffle_dataset = True\n",
    "random_seed = 42\n",
    "\n",
    "dataset_size = len(final_dataset)\n",
    "indices = list(range(dataset_size))\n",
    "split = int(np.floor(validation_split*dataset_size))\n",
    "if shuffle_dataset: \n",
    "    np.random.seed(random_seed)\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "train_indices, val_indices = indices[split:], indices[:split]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "valid_sampler = SubsetRandomSampler(val_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17, 3)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_sampler),len(valid_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_train_loader = DataLoader(dataset=final_dataset, batch_size = batch_size,sampler = train_sampler)\n",
    "final_valid_loader = DataLoader(dataset=final_dataset, batch_size = batch_size,sampler = valid_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch is:  0\n",
      "Image batch dimensions: torch.Size([1, 3, 512, 512])\n",
      "Image label dimensions: torch.Size([1, 512, 512])\n"
     ]
    }
   ],
   "source": [
    "# Checking the dataset\n",
    "for idx,(images, labels) in enumerate(final_train_loader):  \n",
    "    print(\"Batch is: \",idx)\n",
    "    print('Image batch dimensions:', images.shape)\n",
    "    print('Image label dimensions:', labels.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch is:  0\n",
      "Image batch dimensions: torch.Size([1, 3, 512, 512])\n",
      "Image label dimensions: torch.Size([1, 512, 512])\n"
     ]
    }
   ],
   "source": [
    "for idx,(images, labels) in enumerate(final_valid_loader): \n",
    "    print(\"Batch is: \",idx)\n",
    "    print('Image batch dimensions:', images.shape)\n",
    "    print('Image label dimensions:', labels.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAHQAAAB0CAYAAABUmhYnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAAIfklEQVR4nO2dW5bcKAyGf9GzhOR5eg+Z/W9g0ovI88wSpq15AIwk8AXsqrhd+s5JqgvbZczPRRI2JmaGcx/C786Acy4u6M1wQW+GC3ozXNCb8UfPzt++feP39/cHZcXp4ePj419m/m7TuwR9f3/H3z9/AgAIgHZ4GAQCV9tySkknldpOK0fr4+tfhUkFiM7seFLOeMKULoqIUs7kFS/nLe9Z51SXFTW2ye/zNwZCCL9aue0SNP8gGYFsJjVUZbotpi6G/C2fy/52+R1OBVzLew5ZxbBjfJowTQARUs70NVhaqbJyt8qMQauXOiAopcIHilisst2uYbGe5l+A+oX2RdvWbmt6IOBaZkBASNmZpilVNE2uhBAVtd0S9TGStao7WBq63rVbEZvvhNKOKIlLCxfB877y0qWYuRVcE0IIAa0SsdWX1FHtv8vx23QLasexpa7BZpqMwLobqcWXqbn965Z/VTEBOZbL0KqskPaKy1EFe4Wyki8x3F+1apM9kRQ7ty9bAeKnvtSytdH2mXPz/AIQQiAlqh6s7N66z9IlodOWGBJUj6L6M590rchL5pK8zCrT4Jhuu3RiBoUrjZlbMIAACnUlLnsst8uWkFtVudsoYmY12NfdaBn26zRpEjCIkwsQ9EWkE8HOBH0tMRPEIAQwJoBrC5VU9baDScvhWaffyiU7lpVTc+o8i4dmDXfRgfKGQNR3IZeFk7MRAsCMaWJEz6P2yuvWqO2QhxpFrROUNJo/5TFyvy/Z2obg8kEBIYRYTiwHJl1C1m7Idsceb7u7VFn8L0/WzlR9MBE1/bOXIBl0FNK4unJzgTUHbSBmiaEWKjtQW8daaQxOXeyLCilhACAQvSlRWW4WWFflAW4Lzx6iPYlMy3um7LuYM9Gqj5Z9ACW3pj2USR/2QWOo9RrzaddGA/h9S20oD0E5wQ5etZOzxdAYKgdobtQi1Yl8Od/xieS4P+UwYSuE2sdgYEEG5nQEKEdpc7qLuYEQNY+nraiQ9EhPN4qkkPKUyeNK4no32wUBIdjYb0YGTtcZbqGtv+P3FMbLUSBnB6mciCpR83YdDF1m0A/Nf9uZ0XTSrSiQY2BlnLQC+rRLzkOzw0vhKvc3j0ExovQWDaXi0uwr0wPzoW2T2rvZIxQfFdgXUbIMdLltc+cmofSLQEVUCotzqi0GWmi+laSO6rqkZ8HqI8Z/qXJrWgyNoeVnTQDQu9uTKcK27ulo0T/BLT5VN+tu52NIwXyAQfSg2Rb5WW1wHkCcdgMRaMNAGjKKZleFRZp3tw9nT6zokPefNXQL9wnMXe86B+6c3xtddE6B9pX04Rut94aknIMkh2JrZBuM5aqHEryZPguizSfrDs225HlRj90+ifzUwEpx+5TIV4PXb70+ECmKf+0cq50nMXTXX/mLfN7zYhy4pwibUQvn+QzNtkQYFN5Ozo5zlOExlPITR86lGPJDyWO3l2XQKIr3vTjXY2wM9e72shx+nNC5FmOPE7rveVn6lfHGeWn6W+ibt84rQz038RJ55PZCfDDzXzbRm9vNcEFvhgt6M1zQm+GC3gwX9Ga4oDfDBb0ZLujN+P2C7lli0tnNwLMtJ2HvFyb4CnIn0NdCf/w4djaaH3NsNEpvpmfwxC5366FDb55n8LQut9xT1r/CpLOfvhb68XHwdFSMIDuAOqfwXCt3UTdvpWfxPEGXFqF3TuWpRtHW48d+7/Zx+gUdKPT8vk1MU0lsOJ3uhx5nzA/tEFUvup1u0p4aYjaPc3Oply5Bo5x9RcxIC0xOcalQZi433uvFd5tRwK2VsxzNwH25+dmW8ZNS+m8W265VaE85fqqXo/9RiHkcHBM1v105r6NkDaHW+1+c/Qy9zA4Tx6owUOrW8Jm/JrvJroPk9DH2wC9P2Ii0D/xoaa3eQscZe/rMrgnH55gtzOL33RIaYnhpOJ4+68Q9B+/Z16yr7OxncLZlXgdFD4r2FREWmb53sPS+t4vxhacIMfIzz57QeYOfd7nDHFjrj6JxJP2O0WAsmX9uFQ1zbGm47MLILXs1XROsOWfqFE5e6y+SVluQxlGvAltjZ477ubCG5YI71OXmb/z5qb53CTB3r2IM5tZ2uLCJtYe0B1uosWyJwJ//oZoj6RLWB809bD1xP7yidYUVNVu9Z/LimscI3cmCYu0nq5aKc7vJl58k3TYoxiNFjW1xKExjal5O+8zS1691eiHymzi2GX4RT12e4gU9lCayP5MFfFYAX5/uheDd1zvQ5ZbaEg3Q2gQlkG6tebvfBTZACuDs5ECXW7/ww2QDeU6siOr0wixdwm2G38FNKG/dZrMt7yEtXu2rOlswT5imNO/cwdjijaX9LU6gVBnMFrB3uwtIEz6FbhbKam04HXRbijdKqF+sLvdRJydKMzQuao0x3YkWgwhrpTcY+qNqLK3vq9UBwrwvA837cl+XVBbzhAQDmADmoXo/NMGt6xJVbmH+LtOlvPF9o+Hlw30Mju56quqkynJZzdO7XD1lybNMy6fmujt2y1cUGZuK39xplys6/pqPOQOtKIZ1afK7Rsvoy/mOwRcljo9c9Vz6U0ot7817wPRZLWodWLDtsoyrKVufny8pqg0UFOFojsRlCLKXy3udPMEtxbRTl3q7zpr9H0Rmgvz+zPc0A5ClQdW3ugz3VP3hNRZkwE8aQTFNOjZFXpmKeQulB5gY4ebhweKG8Nxf6cbRmslfjp63GL4FxbbQdueKqgMpnYjMZLHwmDk+P3MzXXU3K60LwAZS9VClC2JL2sFXZenM1CfJba/V6bamNUldQuyKWTwY9bWpg+tciVY8BRtUZbEX5r2W6H2JwD8Afu0+wHkkfzLzd5vYJahzfX7/4o3OqbigN8MFvRku6M1wQW+GC3ozXNCb4YLeDBf0ZvwP40Ezs/8t/lkAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 360x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataiter = iter(final_train_loader)\n",
    "images, labels = dataiter.next()\n",
    "images = images.numpy()\n",
    "\n",
    "fig = plt.figure(figsize=(5,4))\n",
    "\n",
    "for idx in np.arange(4):\n",
    "    ax = fig.add_subplot(2, 2, idx+1, xticks=[], yticks=[])\n",
    "    plt.imshow(np.transpose(images[idx], (1, 2, 0)).astype(np.uint8))\n",
    "    plt.savefig(\"img\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = UNet()\n",
    "# print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Unet(nn.Module):\n",
    "    def contracting_block(self,in_channels,out_channels,kernel_size = 5):\n",
    "        block = nn.Sequential(\n",
    "            nn.Conv2d(in_channels = in_channels,out_channels = out_channels,kernel_size = kernel_size,padding = 2),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(out_channels,eps = 1e-03, momentum = 0.99),\n",
    "            nn.Conv2d(in_channels = out_channels,out_channels = out_channels,kernel_size = kernel_size, padding = 2),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(out_channels,eps = 1e-03, momentum = 0.99)\n",
    "        )\n",
    "        return block\n",
    "      \n",
    "    def expansive_block(self,in_channels,mid_channel,out_channels,kernel_size = 5):\n",
    "        block = nn.Sequential(\n",
    "            nn.Conv2d(in_channels = in_channels,out_channels = mid_channel,kernel_size = kernel_size,padding = 2 ),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(mid_channel,eps = 1e-03,momentum = 0.99),\n",
    "            nn.Conv2d(in_channels = mid_channel, out_channels = mid_channel,kernel_size = kernel_size, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(mid_channel),\n",
    "            nn.ConvTranspose2d(in_channels = mid_channel,out_channels = out_channels,kernel_size = 2,stride = 2,padding = 0,output_padding=0)\n",
    "        ) \n",
    "        return block\n",
    "    \n",
    "    def final_block(self,in_channels,mid_channels,out_channels,kernel_size = 5):\n",
    "        block = nn.Sequential(\n",
    "            nn.Conv2d(in_channels = in_channels,out_channels = mid_channels,kernel_size = kernel_size,padding = 2),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(mid_channels,eps = 1e-03,momentum = 0.99),\n",
    "            nn.Conv2d(in_channels = mid_channels,out_channels = mid_channels,kernel_size = kernel_size,padding = 2),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(mid_channels,eps = 1e-03,momentum = 0.99),\n",
    "            nn.Conv2d(kernel_size = 1,in_channels=mid_channels, out_channels=out_channels),\n",
    "            nn.Softmax(),\n",
    "            \n",
    "        )    \n",
    "        return block\n",
    "    \n",
    "    def __init__(self,in_channel,out_channel):\n",
    "        super(Unet,self).__init__()\n",
    "        self.conv_encode1 = self.contracting_block(in_channels = in_channel,out_channels = 16)\n",
    "        self.conv_maxpool1 = nn.MaxPool2d(kernel_size=2,stride = 2)\n",
    "        self.conv_encode2 = self.contracting_block(16, 32)\n",
    "        self.conv_maxpool2 = nn.MaxPool2d(kernel_size=2, stride = 2)\n",
    "        self.conv_encode3 = self.contracting_block(32, 64)\n",
    "        self.conv_maxpool3 = nn.MaxPool2d(kernel_size=2, stride = 2)\n",
    "        self.conv_encode4 = self.contracting_block(64,128)\n",
    "        self.conv_maxpool4 = nn.MaxPool2d(kernel_size=2, stride = 2)\n",
    "        self.bottleneck = nn.Sequential(\n",
    "                            nn.Conv2d(kernel_size=5, in_channels=128, out_channels=256,padding = 2),\n",
    "                            nn.ReLU(),\n",
    "                            nn.BatchNorm2d(256),\n",
    "                            nn.Dropout(0.2),\n",
    "                            nn.Conv2d(kernel_size=5, in_channels=256, out_channels=256, padding = 2),\n",
    "                            nn.ReLU(),\n",
    "                            nn.BatchNorm2d(256),\n",
    "                            nn.Dropout(0.2),\n",
    "                            nn.ConvTranspose2d(in_channels=256, out_channels=128, kernel_size=2, stride=2, padding=0, output_padding=0)\n",
    "                            )\n",
    "        \n",
    "        self.conv_decode3 = self.expansive_block(256, 128, 64)\n",
    "        self.conv_decode2 = self.expansive_block(128, 64, 32)\n",
    "        self.conv_decode1 = self.expansive_block(64,32,16)\n",
    "        self.final_layer = self.final_block(16, 16, out_channel)\n",
    "        \n",
    "               \n",
    "    def crop_and_concat(self, upsampled, bypass, crop=False):\n",
    "        if crop:\n",
    "            c = (bypass.size()[2] - upsampled.size()[2]) // 2\n",
    "            bypass = F.pad(bypass, (-c, -c, -c, -c))\n",
    "        return torch.cat((upsampled, bypass), 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Encode\n",
    "        encode_block1 = self.conv_encode1(x)\n",
    "        encode_pool1 = self.conv_maxpool1(encode_block1)\n",
    "        encode_block2 = self.conv_encode2(encode_pool1)\n",
    "        encode_pool2 = self.conv_maxpool2(encode_block2)\n",
    "        encode_block3 = self.conv_encode3(encode_pool2)\n",
    "        encode_pool3 = self.conv_maxpool3(encode_block3)\n",
    "        encode_block4 = self.conv_encode4(encode_pool3)\n",
    "        encode_pool4 = self.conv_maxpool4(encode_block4)\n",
    "        # Bottleneck\n",
    "        bottleneck1 = self.bottleneck(encode_pool4)\n",
    "        # Decode\n",
    "        decode_block3 = self.crop_and_concat(bottleneck1, encode_block4, crop=True)\n",
    "        cat_layer3 = self.conv_decode3(decode_block3)\n",
    "        decode_block2 = self.crop_and_concat(cat_layer3, encode_block3, crop=True)\n",
    "        cat_layer2 = self.conv_decode2(decode_block2)\n",
    "        decode_block1 = self.crop_and_concat(cat_layer2, encode_block2, crop=True)\n",
    "        cat_layer1 = self.conv_decode1(decode_block1)\n",
    "        final_layer = self.final_layer(cat_layer1)\n",
    "        return  final_layer\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unet(\n",
      "  (conv_encode1): Sequential(\n",
      "    (0): Conv2d(3, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (1): ReLU()\n",
      "    (2): BatchNorm2d(16, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "    (3): Conv2d(16, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (4): ReLU()\n",
      "    (5): BatchNorm2d(16, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (conv_maxpool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv_encode2): Sequential(\n",
      "    (0): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (1): ReLU()\n",
      "    (2): BatchNorm2d(32, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "    (3): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (4): ReLU()\n",
      "    (5): BatchNorm2d(32, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (conv_maxpool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv_encode3): Sequential(\n",
      "    (0): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (1): ReLU()\n",
      "    (2): BatchNorm2d(64, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "    (3): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (4): ReLU()\n",
      "    (5): BatchNorm2d(64, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (conv_maxpool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv_encode4): Sequential(\n",
      "    (0): Conv2d(64, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (1): ReLU()\n",
      "    (2): BatchNorm2d(128, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "    (3): Conv2d(128, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (4): ReLU()\n",
      "    (5): BatchNorm2d(128, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (conv_maxpool4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (bottleneck): Sequential(\n",
      "    (0): Conv2d(128, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (1): ReLU()\n",
      "    (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (3): Dropout(p=0.2, inplace=False)\n",
      "    (4): Conv2d(256, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (5): ReLU()\n",
      "    (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (7): Dropout(p=0.2, inplace=False)\n",
      "    (8): ConvTranspose2d(256, 128, kernel_size=(2, 2), stride=(2, 2))\n",
      "  )\n",
      "  (conv_decode3): Sequential(\n",
      "    (0): Conv2d(256, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (1): ReLU()\n",
      "    (2): BatchNorm2d(128, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "    (3): Conv2d(128, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (4): ReLU()\n",
      "    (5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): ConvTranspose2d(128, 64, kernel_size=(2, 2), stride=(2, 2))\n",
      "  )\n",
      "  (conv_decode2): Sequential(\n",
      "    (0): Conv2d(128, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (1): ReLU()\n",
      "    (2): BatchNorm2d(64, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "    (3): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (4): ReLU()\n",
      "    (5): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): ConvTranspose2d(64, 32, kernel_size=(2, 2), stride=(2, 2))\n",
      "  )\n",
      "  (conv_decode1): Sequential(\n",
      "    (0): Conv2d(64, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (1): ReLU()\n",
      "    (2): BatchNorm2d(32, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "    (3): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (4): ReLU()\n",
      "    (5): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): ConvTranspose2d(32, 16, kernel_size=(2, 2), stride=(2, 2))\n",
      "  )\n",
      "  (final_layer): Sequential(\n",
      "    (0): Conv2d(16, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (1): ReLU()\n",
      "    (2): BatchNorm2d(16, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "    (3): Conv2d(16, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (4): ReLU()\n",
      "    (5): BatchNorm2d(16, eps=0.001, momentum=0.99, affine=True, track_running_stats=True)\n",
      "    (6): Conv2d(16, 4, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (7): Softmax(dim=None)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "unet = Unet(in_channel=3,out_channel=4)\n",
    "print(unet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move models to CUDA\n",
    "if train_on_gpu:\n",
    "    unet = unet.cuda()\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(unet.parameters(), lr = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precompute_for_images(masks):\n",
    "    masks = masks.cpu()\n",
    "    cls = masks.unique()\n",
    "    res = torch.stack([torch.where(masks==cls_val,torch.tensor(1),torch.tensor(0)) for cls_val in cls])\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/swastik/opt/lib/python3.7/site-packages/ipykernel_launcher.py:26: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-e80993ed5168>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;31m# train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m model_scratch = train(20, final_train_loader, unet, optimizer, \n\u001b[0;32m---> 73\u001b[0;31m                       criterion, train_on_gpu, 'model_scratch.pt')\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-47-e80993ed5168>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(n_epochs, loaders, model, optimizer, criterion, use_cuda, save_path)\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0mtemp_target\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprecompute_for_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp_target\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0mlogp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m             \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_weight_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp_target\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m             \u001b[0mweighted_logp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlogp\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0mweighted_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweighted_logp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-27-fe2ce39dcdb2>\u001b[0m in \u001b[0;36mmake_weight_map\u001b[0;34m(masks)\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mbounds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_boundaries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'inner'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mX2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbounds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0mxSum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mX1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0mySum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mY2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mY1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mdistMap\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxSum\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mySum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "def train(n_epochs, loaders, model, optimizer, criterion, use_cuda, save_path):\n",
    "    \"\"\"returns trained model\"\"\"\n",
    "    # initialize tracker for minimum validation loss\n",
    "    valid_loss_min = np.Inf \n",
    "    \n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        # initialize variables to monitor training and validation loss\n",
    "        train_loss = 0.0\n",
    "        valid_loss = 0.0\n",
    "        \n",
    "        ###################\n",
    "        # train the model #\n",
    "        ###################\n",
    "        model.train()\n",
    "        for batch_idx, (data, target) in enumerate(final_train_loader):\n",
    "            # move to GPU\n",
    "            if use_cuda:\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            ## Shape of target B*H*W\n",
    "            temp_target = torch.squeeze(target)\n",
    "            # mask_to_class converts target in the form of H*W\n",
    "            temp_target = mask_to_class(temp_target)\n",
    "            ## make_weight_map accepts A 3D array of shape (n_masks, image_height, image_width) which is converted by\n",
    "            ## precompute_for_images(temp_target)\n",
    "            temp_target = precompute_for_images(temp_target)\n",
    "            ## precompute_for_images returns the shape H*W\n",
    "            logp = F.log_softmax(output)\n",
    "            weights = make_weight_map(temp_target)\n",
    "            weighted_logp = (logp * weights).view(batch_size, -1)\n",
    "            weighted_loss = weighted_logp.sum(1) / weights.view(batch_size, -1).sum(1)\n",
    "            weighted_loss = -1* weighted_loss.mean()\n",
    "            loss = weighted_loss\n",
    "            #loss = criterion(output,target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss = train_loss + ((1 / (batch_idx + 1)) * (loss.data - train_loss))\n",
    "            ## find the loss and update the model parameters accordingly\n",
    "            ## record the average training loss, using something like\n",
    "            ## train_loss = train_loss + ((1 / (batch_idx + 1)) * (loss.data - train_loss))\n",
    "            \n",
    "        ######################    \n",
    "        # validate the model #\n",
    "        ######################\n",
    "        model.eval()\n",
    "        for batch_idx, (data, target) in enumerate(final_valid_loader):\n",
    "            # move to GPU\n",
    "            if use_cuda:\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "            ## update the average validation loss\n",
    "            output = model(data)\n",
    "            loss = criterion(output,target)\n",
    "            valid_loss +=  valid_loss + ((1 / (batch_idx + 1)) * (loss.data - valid_loss))\n",
    "            \n",
    "        # print training/validation statistics \n",
    "        print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
    "            epoch, \n",
    "            train_loss,\n",
    "            valid_loss\n",
    "            ))\n",
    "        \n",
    "        ## TODO: save the model if validation loss has decreased\n",
    "        if valid_loss <= valid_loss_min:\n",
    "            print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "            valid_loss_min,valid_loss))\n",
    "            torch.save(model.state_dict(), 'model_scratch.pt')\n",
    "            valid_loss_min = valid_loss\n",
    "    # return trained model\n",
    "    return model\n",
    "\n",
    "\n",
    "# train the model\n",
    "model_scratch = train(20, final_train_loader, unet, optimizer, \n",
    "                      criterion, train_on_gpu, 'model_scratch.pt')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model that got the best validation accuracy\n",
    "model = Unet(in_channel=3,out_channel=4)\n",
    "model.load_state_dict(torch.load('model_scratch.pt', map_location=lambda storage, loc: storage))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.FloatTensor\n",
      "torch.LongTensor\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/swastik/opt/lib/python3.7/site-packages/ipykernel_launcher.py:15: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "# Set properties\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "batch_size = 1\n",
    "out_channels = 4\n",
    "W = 10\n",
    "H = 10\n",
    "\n",
    "# Initialize logits etc. with random\n",
    "logits = torch.FloatTensor(batch_size, out_channels, H, W).normal_()\n",
    "target = torch.LongTensor(batch_size, H, W).random_(0, out_channels)\n",
    "weights = torch.FloatTensor(batch_size, 1, H, W).random_(1, 3)\n",
    "\n",
    "# Calculate log probabilities\n",
    "logp = F.log_softmax(logits)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 10, 10])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 10, 10])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Gather log probabilities with respect to target\n",
    "logp = logp.gather(1, target.view(batch_size, 1, H, W))\n",
    "\n",
    "logp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 10, 10])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 5.218061\n",
      "\n",
      "\n",
      "Test Accuracy: 96% (5069412/5242880)\n"
     ]
    }
   ],
   "source": [
    "# Multiply with weights\n",
    "weighted_logp = (logp * weights).view(batch_size, -1)\n",
    "\n",
    "# Rescale so that loss is in approx. same interval\n",
    "weighted_loss = weighted_logp.sum(1) / weights.view(batch_size, -1).sum(1)\n",
    "\n",
    "# Average over mini-batch\n",
    "weighted_loss = -1. * weighted_loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'torch.LongTensor'"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512, 512])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: (0, 0, 0), 1: (0, 0, 255), 2: (255, 0, 0), 3: (255, 255, 255)}"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512, 512])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([0, 1, 2, 3]), tensor([262136,      5,      1,      2]))\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([0]), tensor([262144]))\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 512, 512])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 512, 512])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x12d08ef98>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set properties\n",
    "batch_size = 10\n",
    "out_channels = 2\n",
    "W = 10\n",
    "H = 10\n",
    "\n",
    "# Initialize logits etc. with random\n",
    "logits = torch.FloatTensor(batch_size, out_channels, H, W).normal_()\n",
    "target = torch.LongTensor(batch_size, H, W).random_(0, out_channels)\n",
    "weights = torch.FloatTensor(batch_size, 1, H, W).random_(1, 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 2, 10, 10])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 10, 10])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 1, 10, 10])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
